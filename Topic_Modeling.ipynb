{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df_validation = pd.read_csv('D:/dataaa/cnn_dailymail/validation.csv')\n",
    "df_train = pd.read_csv('D:/dataaa/cnn_dailymail/train.csv')\n",
    "df_test = pd.read_csv('D:/dataaa/cnn_dailymail/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['article'] = df_train['article'].str.lower()\n",
    "df_train['highlights'] = df_train['highlights'].str.lower()\n",
    "\n",
    "df_validation['highlights'] = df_validation['highlights'].str.lower()\n",
    "df_validation['article'] = df_validation['article'].str.lower()\n",
    "\n",
    "df_test['article'] = df_test['article'].str.lower()\n",
    "df_test['highlights'] = df_test['highlights'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "    \n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df_train['article'] = df_train['article'].apply(remove_punctuation)\n",
    "df_train['highlights'] = df_train['highlights'].apply(remove_punctuation)\n",
    "\n",
    "df_validation['article'] = df_validation['article'].apply(remove_punctuation)\n",
    "df_validation['highlights'] = df_validation['highlights'].apply(remove_punctuation)\n",
    "\n",
    "df_test['article'] = df_test['article'].apply(remove_punctuation)\n",
    "df_test['highlights'] = df_test['highlights'].apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train['article'] = df_train['article'].apply(word_tokenize)\n",
    "df_train['highlights'] = df_train['highlights'].apply(word_tokenize)\n",
    "\n",
    "df_validation['article'] = df_validation['article'].apply(word_tokenize)\n",
    "df_validation['highlights'] = df_validation['highlights'].apply(word_tokenize)\n",
    "\n",
    "df_test['article'] = df_test['article'].apply(word_tokenize)\n",
    "df_test['highlights'] = df_test['highlights'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['id'], inplace=True)\n",
    "df_validation.drop(columns=['id'], inplace=True)\n",
    "df_test.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "df_train['article'] = df_train['article'].apply(remove_stopwords)\n",
    "df_train['highlights'] = df_train['highlights'].apply(remove_stopwords)\n",
    "\n",
    "df_validation['article'] = df_validation['article'].apply(remove_stopwords)\n",
    "df_validation['highlights'] = df_validation['highlights'].apply(remove_stopwords)\n",
    "\n",
    "df_test['article'] = df_test['article'].apply(remove_stopwords)\n",
    "df_test['highlights'] = df_test['highlights'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "df_train['article'] = df_train['article'].apply(lemmatize)\n",
    "df_train['highlights'] = df_train['highlights'].apply(lemmatize)\n",
    "\n",
    "df_validation['article'] = df_validation['article'].apply(lemmatize)\n",
    "df_validation['highlights'] = df_validation['highlights'].apply(lemmatize)\n",
    "\n",
    "df_test['article'] = df_test['article'].apply(lemmatize)\n",
    "df_test['highlights'] = df_test['highlights'].apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate the articles and highlights for topic modeling\n",
    "train_corpus = df_train['article'].apply(lambda x: ' '.join(x)) + df_train['highlights'].apply(lambda x: ' '.join(x))\n",
    "validation_corpus = df_validation['article'].apply(lambda x: ' '.join(x)) + df_validation['highlights'].apply(lambda x: ' '.join(x))\n",
    "test_corpus = df_test['article'].apply(lambda x: ' '.join(x)) + df_test['highlights'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data to TF-IDF matrices for each dataset\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# Fit and transform on train set\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Transform validation and test sets\n",
    "tfidf_matrix_validation = tfidf_vectorizer.transform(validation_corpus)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(test_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Latent Dirichlet Allocation (LDA) for each dataset\n",
    "num_topics = 5  \n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform on train set\n",
    "lda_train = lda.fit_transform(tfidf_matrix_train)\n",
    "df_train['topic'] = lda_train.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test set\n",
    "\n",
    "lda_test = lda.transform(tfidf_matrix_test)\n",
    "df_test['topic'] = lda_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform validation\n",
    "lda_validation = lda.transform(tfidf_matrix_validation)\n",
    "df_validation['topic'] = lda_validation.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  article  \\\n",
      "4       [fleetwood, team, still, 100, record, sky, bet...   \n",
      "8       [number, job, description, waiting, darren, fl...   \n",
      "13      [louis, van, gaal, said, option, substitute, p...   \n",
      "21      [everton, still, looking, add, two, new, playe...   \n",
      "24      [glen, johnson, look, destined, leave, anfield...   \n",
      "...                                                   ...   \n",
      "287058  [frank, lampard, brought, curtain, 13year, che...   \n",
      "287060  [cnn, late, sergio, ramos, goal, earned, real,...   \n",
      "287100  [wayne, rooney, took, young, child, onto, wemb...   \n",
      "287102  [real, madrid, looking, extend, 21game, winnin...   \n",
      "287107  [neil, ashton, follow, neilashton, marco, reus...   \n",
      "\n",
      "                                               highlights  topic  \n",
      "4       [fleetwood, top, league, one, 20, win, scuntho...      1  \n",
      "8       [tony, pulis, belief, saido, berahino, look, d...      1  \n",
      "13      [manchester, united, beat, southampton, 21, st...      1  \n",
      "21      [everton, looking, add, two, new, player, squa...      1  \n",
      "24      [full, back, scored, late, winner, liverpool, ...      1  \n",
      "...                                                   ...    ...  \n",
      "287058  [fabregas, disappointed, lack, interest, shown...      1  \n",
      "287060  [sergio, ramos, headed, goal, earns, real, mad...      1  \n",
      "287100  [wayne, rooney, win, 100th, cap, slovenia, wem...      1  \n",
      "287102  [real, madrid, beat, san, lorenzo, club, world...      1  \n",
      "287107  [manchester, united, need, decide, wayne, roon...      1  \n",
      "\n",
      "[30120 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_train_topic_1 = df_train[df_train['topic'] == 1]\n",
    "print(df_train_topic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 article  \\\n",
      "2      [dougie, freedman, verge, agreeing, new, twoye...   \n",
      "3      [liverpool, target, neto, also, wanted, psg, c...   \n",
      "8      [1120pm, former, world, champion, ken, doherty...   \n",
      "12     [england, captain, alastair, cook, completed, ...   \n",
      "16     [arsenal, midfield, trio, jack, wilshere, mike...   \n",
      "...                                                  ...   \n",
      "11459  [rarely, headline, matchday, magazine, prophet...   \n",
      "11464  [vincent, kompany, emerged, injury, doubt, man...   \n",
      "11467  [saviour, english, football, turn, longhaired,...   \n",
      "11468  [blackpool, talk, sign, austria, defender, tho...   \n",
      "11488  [brook, lopez, dominated, twin, brother, robin...   \n",
      "\n",
      "                                              highlights  topic  \n",
      "2      [nottingham, forest, close, extending, dougie,...      1  \n",
      "3      [fiorentina, goalkeeper, neto, linked, liverpo...      1  \n",
      "8      [reanne, evans, faced, ken, doherty, world, ch...      1  \n",
      "12     [alastair, cook, completed, century, second, m...      1  \n",
      "16     [arsenal, take, stoke, city, barclays, 21, pre...      1  \n",
      "...                                                  ...    ...  \n",
      "11459  [ander, herrera, fired, man, united, victory, ...      1  \n",
      "11464  [vincent, kompany, assessed, 24, hour, old, tr...      1  \n",
      "11467  [mauricio, pochettino, gathered, considerable,...      1  \n",
      "11468  [thomas, piermayr, training, blackpool, week, ...      1  \n",
      "11488  [brooklyn, net, beat, portland, trail, blazer,...      1  \n",
      "\n",
      "[2379 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test_topic_1 = df_test[df_test['topic'] == 1]\n",
    "print(df_test_topic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 article  \\\n",
      "3      [avid, rugby, fan, prince, harry, could, barel...   \n",
      "5      [team, lowest, transfer, outlay, season, rose,...   \n",
      "8      [ronda, rousey, recorded, fastestever, finish,...   \n",
      "9      [celtic, defender, virgil, van, dijk, admits, ...   \n",
      "12     [radamel, falcao, reduced, tear, manchester, u...   \n",
      "...                                                  ...   \n",
      "13359  [make, mistake, first, trophy, celtic, manager...   \n",
      "13361  [match, martin, â€™, neill, needed, win, convinc...   \n",
      "13362  [per, mertesacker, say, frank, team, meeting, ...   \n",
      "13364  [mo, farah, nationality, called, question, spa...   \n",
      "13365  [wolf, kept, promotion, hope, alive, routine, ...   \n",
      "\n",
      "                                              highlights  topic  \n",
      "3      [prince, harry, attendance, england, crunch, m...      1  \n",
      "5      [stoke, city, beat, everton, 20, move, eighth,...      1  \n",
      "8      [ronda, rousey, submitted, cat, zingano, via, ...      1  \n",
      "9      [celtic, defeated, dundee, united, 20, scottis...      1  \n",
      "12     [silvano, espindola, spoken, radamel, falcaos,...      1  \n",
      "...                                                  ...    ...  \n",
      "13359  [ronny, deilas, celtic, side, 15th, scottish, ...      1  \n",
      "13361  [slawomir, peszko, fired, poland, lead, 26thmi...      1  \n",
      "13362  [arsenal, face, monaco, second, leg, last16, t...      1  \n",
      "13364  [mo, farah, broke, european, halfmarathon, rec...      1  \n",
      "13365  [wolf, three, point, playoff, place, eight, ga...      1  \n",
      "\n",
      "[2957 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_validation_topic_1 = df_validation[df_validation['topic'] == 1]\n",
    "print(df_validation_topic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted data to an Excel file\n",
    "output_excel_file = 'D:/dataaa/df_validation_topic_1.xlsx'\n",
    "df_validation_topic_1.to_excel(output_excel_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted data from training set to an Excel file\n",
    "output_train_excel_file = 'D:/dataaa/df_train_topic_1.xlsx'\n",
    "df_train_topic_1.to_excel(output_train_excel_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted data from test set to an Excel file\n",
    "output_test_excel_file = 'D:/dataaa/df_test_topic_1.xlsx'\n",
    "df_test_topic_1.to_excel(output_test_excel_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on validation set: 15917283.596167924\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity on validation set:\", lda.perplexity(tfidf_matrix_validation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
